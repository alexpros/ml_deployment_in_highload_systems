{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, some models of various types are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch script model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "resnet18 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "resnet18.eval()  # Set the model to evaluation mode\n",
    "scripted_model = torch.jit.script(resnet18)\n",
    "scripted_model.save(\"triton_repository/pytorch_model/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doblakov/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/doblakov/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "ResNet18_Weights = models.resnet18(pretrained=True)\n",
    "ResNet18_Weights.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Dummy input tensor for the ONNX export (batch size = 1, 3 color channels, 224x224 image)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Path to save the ONNX model\n",
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    ResNet18_Weights,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    export_params=True,               # Store the trained parameters in the model file\n",
    "    opset_version=11,                 # ONNX opset version\n",
    "    do_constant_folding=True,         # Optimize constant folding for inference\n",
    "    input_names=[\"input\"],            # Input tensor name\n",
    "    output_names=[\"output\"],          # Output tensor name\n",
    "    dynamic_axes={                    # Specify dynamic axes for batch size\n",
    "        \"input\": {0: \"batch_size\"}, \n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"ResNet-18 ONNX model exported successfully to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "bert.eval()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tokenizer(\"Привет, как дела?\" * 100, return_tensors=\"pt\", max_length=512, padding=\"max_length\")\n",
    "dummy_input = (dummy_input[\"input_ids\"], dummy_input[\"token_type_ids\"], dummy_input[\"attention_mask\"])\n",
    "torch.onnx.export(\n",
    "    bert,\n",
    "    dummy_input,\n",
    "    \"bert.onnx\",\n",
    "    export_params=True,               # Store the trained parameters in the model file\n",
    "    opset_version=17,                 # ONNX opset version\n",
    "    do_constant_folding=True,         # Optimize constant folding for inference\n",
    "    input_names=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],            # Input tensor name\n",
    "    output_names=[\"output\"],          # Output tensor name\n",
    "    dynamic_axes={                    # Specify dynamic axes for batch size and sequence length\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"}, \n",
    "        \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic quantization complete.\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Path to your float32 ONNX model\n",
    "model_fp32 = \"bert.onnx\"\n",
    "# Path where the quantized model will be saved\n",
    "model_quant = \"bert_int8.onnx\"\n",
    "\n",
    "# Apply dynamic quantization on the model weights (e.g., Linear, MatMul operators)\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\n",
    "print(\"Dynamic quantization complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
