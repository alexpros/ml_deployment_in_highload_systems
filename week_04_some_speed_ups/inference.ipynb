{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import tritonclient.grpc as grpcclient\n",
    "import torchvision.transforms as transforms\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "from typing import Callable, List\n",
    "import numpy.typing as npt\n",
    "from tritonclient.utils import InferenceServerException\n",
    "# Create a client\n",
    "client = grpcclient.InferenceServerClient(url=\"localhost:13331\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _async_infer_callback(all_results: List, result, error: InferenceServerException):\n",
    "    if error:\n",
    "        all_results.append(error)\n",
    "    else:\n",
    "        all_results.append(result)\n",
    "\n",
    "\n",
    "def _async_infer_callback_with_latency(\n",
    "    all_results: List,\n",
    "    result,\n",
    "    error: InferenceServerException,\n",
    "    start_time: float,\n",
    "):\n",
    "    # Call the original callback to store the result\n",
    "    _async_infer_callback(all_results, result, error)\n",
    "\n",
    "\n",
    "def default_infer(\n",
    "    model_name: str,\n",
    "    image_numpy: npt.NDArray,\n",
    "    request_id: str = \"0\",\n",
    "    results: List = None,\n",
    ") -> bool:\n",
    "    inputs = [grpcclient.InferInput(\"input\", image_numpy.shape, \"FP32\")]\n",
    "    inputs[0].set_data_from_numpy(image_numpy)\n",
    "    outputs = [grpcclient.InferRequestedOutput(\"output\")]\n",
    "    start_time = time.perf_counter()\n",
    "    return client.infer(\n",
    "        model_name=model_name,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        request_id=request_id,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "async def infer(\n",
    "    model_name: str,\n",
    "    image_numpy: npt.NDArray,\n",
    "    request_id: str = \"0\",\n",
    "    results: List = None,\n",
    ") -> bool:\n",
    "    if results is None:\n",
    "        results = []\n",
    "    \n",
    "    # Create input tensor\n",
    "    inputs = [grpcclient.InferInput(\"input\", image_numpy.shape, \"FP32\")]\n",
    "    inputs[0].set_data_from_numpy(image_numpy)\n",
    "\n",
    "    # Create output tensor\n",
    "    outputs = [grpcclient.InferRequestedOutput(\"output\")]\n",
    "\n",
    "    # Record the start time for this inference request\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Send inference request with our modified callback that records latency\n",
    "    client.async_infer(\n",
    "        callback=partial(\n",
    "            _async_infer_callback_with_latency,\n",
    "            all_results=results,\n",
    "            start_time=start_time,\n",
    "        ),\n",
    "        model_name=model_name,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        request_id=request_id,\n",
    "    )\n",
    "\n",
    "    # In this example, we immediately return True; results are collected in the callback.\n",
    "    return None\n",
    "\n",
    "\n",
    "async def calculate_throughput(\n",
    "    infer_func: Callable, num_cycles: int, *args, **kwargs\n",
    ") -> None:\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Lists to collect inference results and latencies\n",
    "    results: List = []\n",
    "\n",
    "    # Create and gather all async tasks\n",
    "    tasks = [\n",
    "        infer_func(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            request_id=str(i),\n",
    "            results=results,\n",
    "        )\n",
    "        for i in range(num_cycles)\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # Wait until all callbacks have been executed\n",
    "    while len(results) < num_cycles:\n",
    "        await asyncio.sleep(0.01)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Extract model name and batch size from inputs\n",
    "    model_name = args[0]\n",
    "    batch_size = args[1].shape[0]\n",
    "\n",
    "    execution_time_per_cycle = (end_time - start_time) * 1000 / num_cycles  # ms\n",
    "    throughput = batch_size / (execution_time_per_cycle / 1000)  # Images per second\n",
    "\n",
    "    logger.info(f\"Model: {model_name}\")\n",
    "    logger.info(f\"Batch size: {batch_size}\")\n",
    "    logger.info(f\"Execution time per cycle: {execution_time_per_cycle:.2f} ms\")\n",
    "    logger.info(f\"Throughput: {throughput:.2f} requests/second\")\n",
    "    logger.info(\"-\" * 50)\n",
    "\n",
    "    return throughput\n",
    "\n",
    "\n",
    "speed_results: dict[str, float] = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the speed of inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name: \"pytorch_model\"\n",
       "model_version: \"1\"\n",
       "id: \"0\"\n",
       "outputs {\n",
       "  name: \"output\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: 1\n",
       "  shape: 1000\n",
       "}\n",
       "raw_output_contents: \"\\252:\\314\\277QEN?%\\266\\266?4\\234\\216?q\\323Z?PS\\370\\276\\231C;?\\276A\\024>\\030\\227\\244\\277q\\370\\217\\276g\\003\\273?\\203ZE@\\242S\\343?TaJ@\\032P9@d\\330\\272?\\327{\\002@\\266F\\016?=\\323\\330?\\365\\267\\021@\\333\\002M?R \\023@\\242\\234\\023@\\006S\\370?\\200\\033q?n8H>\\256\\004 ?#_3?\\301\\036h\\276\\306\\234\\206\\277\\030\\236-\\277\\357\\n\\263?\\27377\\277\\266O&?V\\014\\371?\\313\\001-\\2772\\2251\\277m\\3773\\276\\232\\313\\266?\\362m\\321>\\322\\256,@\\332\\245z?\\375\\256\\361?as\\315>*\\013\\342?\\023ip>\\263b\\021@Ts\\200\\276\\017M\\250\\275\\376\\010\\355\\275G+\\272?\\003\\320\\265\\277\\351$\\305?2W\\265?B\\213\\203>\\272r\\201?EF\\305>\\343\\321C>8\\007\\257?\\\\I\\363?\\276j\\214?@\\333\\033?9\\n\\253\\275\\367eZ\\277\\315\\336.?\\221p\\007@eIX\\276^\\336\\202\\277\\263\\027\\003\\275\\315\\260\\032@\\037\\360\\r@\\225Q\\\"@8\\210V?/[,@\\341\\254{?\\375\\037-@\\215\\037V?\\034\\017\\001@\\014\\004\\206@\\004kb@6m\\002\\276A\\366\\315?I`P?\\t\\200\\277\\277\\005e\\016\\277\\002\\266\\366?SD1>\\336O%@D\\264p?\\313\\234\\036@z\\r\\021>\\220\\3243?\\307\\004`@\\216\\377\\233?\\344|E@\\362\\246\\221<\\305k\\346?\\264%\\230\\277?\\323\\030\\276\\271\\271L>M\\002\\013?\\353\\217\\374\\277\\365\\314\\265\\277\\004RS?\\024wT\\275\\326_\\220\\277\\241\\037\\275\\277\\230\\n\\367?\\203\\315\\356\\277\\223l\\317\\276\\273_\\225\\277\\2343\\272@]g\\306?\\262\\301\\245?\\200\\004\\352?/\\365\\000\\300\\023,\\302\\277Mq\\330>\\305)\\253\\276$4K\\277%\\305\\021?[\\373\\353\\277L\\\"\\\"\\277{\\355\\200\\277\\270\\226g?\\t3\\205?x\\371\\375?\\211[\\351?\\313\\237\\355?\\377\\251\\326?\\332\\266\\354\\275\\360WW?%\\017\\343?\\350\\002\\254?\\327R\\257?`\\210u\\274\\255\\026]\\276\\353\\3558?r\\024t?\\251M\\252?}t~?{[\\014@\\303h\\374>\\314\\t\\005@\\377\\242\\220?N}}\\276\\344\\001Y?\\245O\\302>\\275Rz\\277}\\024\\374\\275\\262%(\\277\\342}\\306\\276\\2478\\027\\300\\013z >\\nb\\202\\277c7\\033\\277\\305\\253\\213\\277\\347l\\222\\277\\273x\\016\\277\\2638!\\277IQ\\364\\277\\344j\\253\\275G\\250q>\\221l\\342\\276\\356\\021?\\276\\320\\267\\313\\277\\256 +\\276\\261\\260\\024\\300\\023\\303\\360\\276e\\177\\214\\2775\\\\\\237\\277DB\\217>\\212\\014\\021\\277\\0356\\013\\277\\351+j\\277J\\372\\024\\300!c\\310\\277\\264\\360\\016\\300\\305\\206+\\276U\\277\\272\\277Ui\\033\\277V\\214\\315\\277\\013;k\\277\\224b\\211\\277\\024b\\252\\277\\333\\263\\236\\277r*\\336\\277\\251e\\261>\\234%\\026\\300\\273\\223=\\277\\\\H\\254\\277\\360+\\214\\277\\362\\315J\\277L{\\007\\300\\230S\\025\\300\\350\\236\\216>\\367&\\246\\276\\317\\214\\r\\300\\2463\\377\\277r\\204\\207\\277\\301|\\024\\300\\004\\372\\222\\277\\02052\\277\\t)\\307>\\312\\202\\333\\274\\201\\231\\320\\277\\'\\300\\307\\277v\\3700\\277\\021T\\231\\277G{\\350>xR\\263\\277\\2350\\313\\277\\321\\270T\\277\\016\\337\\222\\277\\\"%\\322\\277+\\225\\306\\276\\331X\\366\\277N\\246\\212\\276p\\225G\\277\\310\\362\\316\\277s\\371<\\300\\304\\020\\306\\277Eu\\331\\277bT\\202\\275\\324\\017\\330\\277\\n\\343e\\277U \\331\\277\\237\\255\\323\\277-\\315\\\"\\300\\254\\222\\243\\277\\254e\\302\\276N\\262\\337\\277tM\\\\\\277\\324\\207W\\300\\231\\326\\261\\277\\343\\020\\257\\277z\\227\\201\\277\\212{>\\277\\351y\\254\\276\\273\\245@\\277\\307\\331\\262\\277\\241\\306\\236\\277\\211c?\\277\\026\\327\\274\\277\\t\\\"o\\300X\\350\\306=\\377O\\300\\277\\016j\\225\\277\\347\\336)\\277)<\\023\\277\\306#l\\277\\377/#\\277y\\366(\\300\\200\\035\\021\\277\\320\\310D\\277mQR\\300-\\t\\002\\300\\221!\\230\\2773\\265g\\276>\\273x\\2762$\\024\\300h\\231\\271\\277!\\324R\\300\\206\\020\\225>\\312\\3528\\277\\353X\\326\\276\\013\\361R\\277\\251\\\"!\\3008\\215\\033\\300\\323\\303\\330\\277C\\010\\217\\277\\302\\363\\t\\300\\347\\353\\331<\\336\\014v\\276\\016\\006\\370\\277\\227\\356\\036\\277\\200_\\224\\277\\334z\\030\\277W\\317T\\277P\\242\\246<\\224}\\344\\276\\272\\022B>\\372A_=p\\245&\\276\\214\\017\\013\\274\\322\\252\\037\\276\\356\\314\\346\\277\\036\\346\\354\\277Tk\\345\\277\\233;\\003\\300\\007\\0319\\300\\3127\\351\\277\\320\\261\\351\\277\\235.\\227\\277\\257%\\001\\300\\210\\235\\201\\2777\\203x\\277\\255\\322!\\300\\262\\336\\236\\2761\\265\\004\\277AS\\343\\275\\303p6@ \\277\\222?\\337\\377\\036@\\245l\\241?\\025\\026\\300>\\230\\240\\304?\\001\\212\\013@\\201\\260\\017@b\\264#=?\\2714@ \\274\\025@\\2266\\\"@\\305\\256c@L\\212<@%XC@\\316!L@\\316c]@\\264[\\217@\\364\\007B@\\207\\260,@:\\335\\203\\275J\\363\\237\\277G9\\343=\\340\\260O?\\325\\033\\341=\\252\\344\\230?n\\377\\355?\\244_|\\276.\\335,\\277B\\334\\340\\276\\333.|>\\233\\001\\353\\276\\300N\\316\\276N@\\216\\277$W\\\"\\276\\357s\\264\\277\\235Co\\277V+\\252\\277q\\232\\020\\3004N\\002\\300\\315\\233d\\300!@\\024\\300\\371V\\315\\277\\262]\\231\\277\\365\\331\\021\\300\\\"\\254\\212\\277!\\277\\271\\277e.\\002\\300\\\\\\306B\\277\\331\\\\\\324\\275,\\343!\\277\\306#\\236\\277\\034\\211\\357\\276\\226\\343\\214\\277R\\270_\\300\\010\\301\\336>\\305\\333\\n\\277\\317\\235\\246\\273\\340$\\305>u\\363\\251\\277y\\317>?,\\310\\031\\277P^\\302>\\234\\350\\374\\276\\316\\221\\242\\277\\310$\\025\\300p\\342\\330\\276\\000\\317\\330\\277\\233u\\332\\277\\367\\242\\014\\277\\244\\223\\366\\277L\\247\\333\\277\\0308f\\277\\316\\177\\\">\\251\\034\\215\\277\\276\\337\\350\\275\\000j\\334>\\307\\212[\\277\\302\\211\\255\\277\\375\\337\\305\\277\\271\\343$\\277\\237B\\207\\276\\340\\220\\035\\276\\371S\\307\\277\\255D\\021\\300b\\364\\331\\277iC\\374\\277\\374\\373\\006\\300\\376c\\220\\277D`\\366\\276\\325\\031\\215>Y@\\310\\277\\351\\322\\327\\277\\020\\337\\341\\276\\312\\201\\200?/\\271\\317=\\204#\\013\\275\\354\\265\\376\\276I\\303\\344\\275\\344\\232\\007\\300E\\266\\242\\277\\177Q\\325?\\004\\337\\206?.\\375\\250?\\037L\\022@\\347\\263\\257\\2774$\\304\\277\\'D\\014\\300\\372&\\301?R\\264S\\277\\266l$\\276\\332!n>\\303\\002\\'?\\275\\346\\350\\276}\\224\\020\\300j^\\302\\276\\301Q\\313?T#\\226@\\344s\\255?W\\273p?\\030ho>\\226\\035\\334\\275\\217I\\005\\300\\0073\\020\\300)\\321#\\277.\\035\\203?\\026\\n\\357\\276@\\200\\267\\277\\325\\262\\256?\\301\\010\\251\\277?\\006\\243\\277\\002>\\300?\\206\\006\\024\\276n\\032\\203\\276\\007\\244\\354\\2777A?\\277\\24331?\\273\\215q\\275\\203\\230\\314\\277\\021\\325\\342?\\247\\002k?\\200\\027;\\276Z\\375\\347?\\311\\306\\320\\276Kh\\'\\277D\\256.@\\330\\226\\332\\276\\315\\233\\250?\\037G\\\\\\277\\233}f\\277\\327\\334)@\\252Pn\\275\\203X\\376\\276a\\262\\335\\277\\207\\2067@\\013c\\312>\\300\\245\\210>\\361\\215\\265?\\246\\025\\202>\\312\\177\\301?\\213\\364\\006\\277%c\\306?\\370}C>\\326!\\266?\\214\\367\\363\\276 J\\220\\277\\363\\237+\\300\\203\\320\\265\\276\\202\\273\\272\\277\\271z}?E\\330y>\\2759B=k(\\341?*\\260\\207?eQ\\250\\277\\354\\031\\201\\300F[\\333\\276\\371G\\211?\\205i\\223\\276\\272D\\303>1\\211\\367?s\\250\\212\\276<`\\353\\276\\021\\356\\340>\\037\\030\\210?\\2448\\251\\276\\315w\\263?\\311\\340#@\\237\\355\\226\\275\\005\\372\\327=v\\316\\037?\\002\\211\\002?\\312\\342\\265\\277\\227\\377\\366>l{\\205\\277\\310\\336D?n\\237\\237\\277\\304\\002Q\\276,3\\275?\\213\\245\\r\\277]1\\006\\277\\230\\353\\214>&\\262\\225>8>*?\\336\\265\\014?mU\\226>?.\\315?c\\031\\\"\\275%\\3667\\300{lk>\\035\\345\\256\\277h~\\'@\\r\\322\\326\\275\\371\\247\\343=hI\\224>!\\'\\365\\277\\203\\316\\336?Y\\177k\\2773\\th\\277\\217\\217\\233\\277\\024\\363\\234\\277\\247\\035B\\276{\\271\\317?\\313\\037\\264\\277\\246\\235<\\277\\213\\350\\360\\276\\204\\037\\223\\277\\3737\\020>\\212n{\\274\\361>\\324?60T?\\257\\007_\\277\\315\\216\\306?8\\201\\275=18\\244\\277\\213;f\\277f\\003\\330\\276s\\\"\\220?\\001QR\\276\\237lG?\\2627\\351\\274\\222\\017\\311?\\337_\\374\\276F\\2305\\277\\357Z\\276?>\\231\\366<M\\342\\030\\300;4\\003\\277\\223\\367Y@n~\\005\\300N\\002\\206\\276e\\304\\241>\\237\\206G?\\315\\177\\021?\\355&\\262\\277\\304\\3605\\275U}\\274?\\254i]?Szs?\\224\\314\\027\\2774\\265\\032\\277\\266\\372t\\275\\317\\215d@\\322\\262\\367\\277\\266\\261\\215\\277\\336\\rF\\277+\\'i?\\000\\337\\245\\277\\370K\\317\\277\\006\\236\\256\\276\\311i\\n\\300\\353\\263\\227?\\313&\\372\\276\\030\\257\\025@cz\\367\\277\\351C\\264\\277\\330[3\\277\\354\\007>\\277\\377\\035}\\277\\213Z\\030\\300\\t\\340\\222\\277\\342\\221+\\300d\\302\\230>w\\235\\017@\\342:\\000\\275\\\"\\340:\\277\\037\\342 @V\\025\\343\\275U\\023\\031?\\310\\332\\231\\277Q\\213\\316?\\354\\356\\333?\\025+\\216?\\361\\021\\236\\276\\215\\353\\357\\277F\\255\\255?\\017\\204\\311?\\262\\310R\\276\\262\\237\\341\\276\\320\\rS@\\337\\0171>9)\\216\\276\\311M\\025\\300(I\\357?\\033\\255\\\"@\\336\\364\\364?l\\177\\227\\276Y6\\237?\\305\\224\\303\\277\\225\\270\\225?\\031u\\032@\\274p\\013\\300\\206\\243\\350?\\326\\017\\216\\277\\205@\\245>\\263\\014v@\\344\\332]\\277\\374\\256&@\\263\\216\\031@c=\\034@\\001\\033\\244\\277\\247\\3248@4\\245h@\\217\\235\\031\\277\\266]\\024\\277\\001\\337\\t@\\260\\242\\202\\277\\\\B??9\\206\\374?9\\246\\201\\276a\\2242?\\365u\\253?\\021f\\312?\\327\\234h\\300\\2114\\200?~\\310\\203\\275\\013\\320\\302=\\327\\027\\006\\277mm%\\277\\004\\226x?Le\\250>\\214e\\345\\277>\\365\\333>\\224\\332\\215@.\\234-\\277\\316\\247@\\276;\\236\\312>\\342\\274N\\277\\004P\\214\\276K\\215\\342?\\026\\234\\245\\277\\306Q3\\277\\367\\254\\236\\276\\265S\\301\\277\\246\\255\\031?.g\\262\\277\\254Q2@8\\250k>\\303\\201\\033?\\344\\203c\\277$\\337$\\300\\244\\307\\024@\\334}\\300\\277Z+\\222?R\\010|\\275\\3639\\001?\\202\\314\\'\\274W\\024\\263>\\237\\307\\272\\276\\024g\\313\\277e\\315\\226\\277Qk\\031=\\236e\\002@ut\\024@\\343\\254\\344\\277H\\323\\322\\277;\\235]@\\236Lw\\277\\247\\254p@\\0225B?\\337a\\017@6\\310\\026@\\277/\\253?\\244\\010\\243?M\\206\\276>\\355=\\204\\276\\224\\302)\\300#\\365F?\\365\\266\\263\\277\\'\\262:\\300h}\\264\\276\\367\\374{?1\\254\\237>\\351\\325\\201\\277\\247\\337\\340?\\373\\364\\322?\\013E\\210\\275\\rd\\243\\277\\372\\305\\322>\\302\\204\\353?\\333\\267:@\\370\\271\\201\\277\\205\\257\\276\\277\\210zH?\\363\\023\\006\\300\\320\\267~\\277\\\"\\220\\204\\276\\0171J\\276\\362\\216\\334?&y\\365?\\270c\\241?\\221\\350\\240?\\223\\3215\\277I\\311D@\\252\\247\\302\\277q\\253@?.\\034(\\300\\\\Ur?\\022\\014b?\\236\\n\\037@\\266\\264\\216?\\254\\3311@nf]?\\277\\226M>\\314\\237\\262?u\\362&=*\\\\%?;\\261\\257>\\331Q!\\277\\005\\352\\215\\277g[\\276?\\207\\236\\201\\276\\357\\r\\r@M\\313\\021\\300,\\261\\250?u2\\313?1\\340\\350?\\333\\025\\340>\\206Fm\\300B\\315\\203?\\3741\\346\\275\\272\\241D\\277\\177\\256!\\2776\\375\\333?\\261\\221\\255>\\303\\026\\305?\\020\\372(\\277\\252\\032:?\\273\\3139@\\357\\265t\\276\\311\\344s\\277D\\200\\035@\\302\\313\\322?\\2333=\\275\\227\\004\\352>-\\266|\\277Bt\\002\\300\\\"\\264\\225\\2756\\301G?Z$\\210<x\\353\\360?\\222\\300$\\300\\315\\220E@\\262\\265\\025?/\\007\\030>\\273\\352S\\300\\2310[@(\\265\\234\\276\\323\\377M@\\235\\264\\211?\\257Wp?\\311\\216 @)\\002\\010@#j\\224?\\177\\002\\364\\276\\322\\206\\021?\\304~\\372?\\304}f\\276\\342J\\276\\277\\\"\\337\\014?\\rX7\\2765\\036\\262?\\360\\232\\201@\\347\\203\\201@\\037Z\\273\\277B\\257\\023<\\352\\336\\373\\276\\301\\305\\024\\300\\240b\\223\\276\\354\\025\\277\\277\\343D8?\\243e\\200>\\314j\\365=\\221\\314\\357\\275\\225\\367\\207?\\001}\\273>\\266\\335\\234\\277\\247\\215\\371?\\177\\255H=\\031-\\037\\300\\241u\\007>\\021\\316k\\277m\\260-\\276\\247\\314\\337?b\\203(?r\\274;?\\r\\340\\006\\300\\243L\\372\\276\\031\\240*?\\355:\\240\\277\\320\\357^?Y\\037(@\\2247\\337?N\\305\\325>\\001\\263\\254?\\217\\247\\310?\\203\\314\\025\\300\\207\\317\\025@\\256{\\244\\277\\373x+\\300x8\\306\\276{P\\313\\277\\202\\223:>\\362\\233\\034<\\202W\\331\\277\\263{\\311?TM\\345\\277=\\374\\354?\\215\\242\\013\\300{79\\277\\243\\332d\\277\\014\\2514\\277!\\034u?^`d>\\263\\375\\252\\276Gx\\323\\276x\\270\\013?P\\360\\362\\275\\305\\364\\361>\\205\\212\\225?\\246\\360\\270<\\270z\\021>v\\010\\205\\277\\225\\024\\205@F\\311\\223@)\\357\\t@G\\\\\\203\\277\\257\\323\\013?v\\265*?.\\034\\220\\275\\261\\330q?d\\0004@y\\031/\\277\\372\\215\\332\\277Us0@\\016\\224\\200\\300?\\201\\331\\277%\\244\\227=\\321t\\331\\275\\335\\t$\\300\\333h\\311>.N\\203?\\302\\370\\320>\\346\\336\\317\\277\\240\\n\\203\\300T\\234\\336\\277d\\212\\251\\277VX\\217?fS\\341<\\036\\353;\\276Me\\016>r\\020\\357?\\373\\3247?\\250H/\\300\\021\\212\\363\\276\\3200\\213\\277\\202\\330\\347\\277\\371\\275\\355\\277m\\212\\232?\\300\\272c?x\\304\\213\\277Ca\\217?\\334#\\307?\\234\\241\\000\\276\\240\\350\\356>\\317\\262\\351\\277\\2072\\340\\277\\351\\370\\306=\\020\\201~\\275\\314\\032\\270\\276\\270\\367\\233\\276\\215\\373\\225@\\323\\3677@\\234\\3748\\277\\221D\\232?U\\227\\315\\276\\357\\367\\235\\277QU\\242?\\264\\353\\020?R\\251p?\\371\\325\\310>z\\2040@X\\342B\\275-A\\307?\\303\\004S\\276\\324\\334\\017\\277Z \\206?MM\\347?_\\312\\032\\276m9\\300?\\315\\323\\355>\\245\\243\\326\\277o\\365=?&\\020*\\277:\\206\\247\\277\\236s\\322\\277a\\006\\234\\277B\\323\\246\\276M,\\220?>\\324\\351?\\034\\252\\221?3c-?:\\254Y\\277\\252E\\247\\277\\353(\\027\\277j\\265X\\300\\033!3>d\\\"\\274=NU\\313?%\\256r\\277\\227\\003\\t\\277 \\023\\234=+!\\342\\277\\234\\213\\365\\277\\'\\270\\235\\277\\327\\325\\004\\300hg\\250>R\\\\n\\277b\\276\\222\\276\\346l\\360\\276\\376\\257\\315\\277\\034Z\\235\\277\\342\\362e\\277\\263\\027\\344\\277\\210}k\\275Ka\\371\\277\\231\\230\\320\\277\\203\\013\\232?aI<?\\213\\262q?\\004\\306\\226?\\035\\314Z?\\264hS\\276\\361\\211\\210?\\305O\\200\\277\\370\\016\\277\\2775g\\233=U\\310\\014\\277\\234\\337\\206\\276\\322\\313\\035\\276\\344!\\026?).\\250\\277\\\\!\\331>\\270A^\\277\\350z\\372\\277(\\207\\242?h`\\237??I\\242?\\245\\231\\\"\\277H\\266\\234\\277j|\\316?\\371\\0263<\\237_\\241\\277Qq\\200\\276m<\\232?\\036\\343\\230?\\'\\007\\377?\\207\\322\\201?+\\260\\242\\277\\030\\226\\320?N\\370;?\\363\\311\\301\\277d\\220\\265=\\273\\306\\236=P\\330\\225>\\234\\213\\006\\277\\350\\212\\370>2\\035\\252?\\013a\\375>\\020e\\330=\\275\\366\\253\\277\\273^\\330\\277\\353\\374<\\300S\\321\\003\\300q\\231\\340\\277}t\\034\\300aF_\\277\\023\\375\\231>\\236\\236\\034@\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "res = default_infer(\"pytorch_model\", image_numpy)\n",
    "res.get_output(\"output\")\n",
    "# res.as_numpy(\"output\")\n",
    "res._result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 20:13:23,986 - Model: pytorch_model\n",
      "2025-02-19 20:13:23,999 - Batch size: 1\n",
      "2025-02-19 20:13:24,003 - Execution time per cycle: 135.77 ms\n",
      "2025-02-19 20:13:24,006 - Throughput: 7.37 requests/second\n",
      "2025-02-19 20:13:24,007 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"pytorch_model\"] = await calculate_throughput(infer, 100, \"pytorch_model\", image_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 23:06:13,504 - Model: onnx_model\n",
      "2025-02-17 23:06:13,507 - Batch size: 1\n",
      "2025-02-17 23:06:13,509 - Execution time per cycle: 162.49 ms\n",
      "2025-02-17 23:06:13,510 - Throughput: 6.15 requests/second\n",
      "2025-02-17 23:06:13,511 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"onnx_model\"] = await calculate_throughput(infer, 10, \"onnx_model\", image_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:50:44,835 - Model: onnx_model\n",
      "2025-02-15 12:50:44,838 - Batch size: 16\n",
      "2025-02-15 12:50:44,838 - Execution time per cycle: 15.48 ms\n",
      "2025-02-15 12:50:44,839 - Throughput: 1033.43 images/second\n",
      "2025-02-15 12:50:44,839 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# search for \"shape: [\" in the logs\n",
    "image_numpy = np.random.rand(16, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"onnx_model_external_bs16\"] = await calculate_throughput(infer, 1000, \"onnx_model\", image_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:50:46,033 - Model: dynamic_batching\n",
      "2025-02-15 12:50:46,035 - Batch size: 1\n",
      "2025-02-15 12:50:46,036 - Execution time per cycle: 1.18 ms\n",
      "2025-02-15 12:50:46,036 - Throughput: 846.81 images/second\n",
      "2025-02-15 12:50:46,036 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# search for \"executing \" in the logs\n",
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"onnx_model_dynamic_batching\"] = await calculate_throughput(infer, 1000, \"dynamic_batching\", image_numpy)\n",
    "\n",
    "# ... why slower than onnx_model with batch 16?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muti-instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:51:39,199 - Model: multi_instance\n",
      "2025-02-15 12:51:39,201 - Batch size: 1\n",
      "2025-02-15 12:51:39,201 - Execution time per cycle: 0.96 ms\n",
      "2025-02-15 12:51:39,202 - Throughput: 1036.53 images/second\n",
      "2025-02-15 12:51:39,204 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# search for \"shape: [\" in the logs\n",
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"onnx_model_multi_instance\"] = await calculate_throughput(infer, 1000, \"multi_instance\", image_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 12:51:17,778 - Model: quantized_model\n",
      "2025-02-15 12:51:17,779 - Batch size: 1\n",
      "2025-02-15 12:51:17,780 - Execution time per cycle: 2.97 ms\n",
      "2025-02-15 12:51:17,780 - Throughput: 337.22 images/second\n",
      "2025-02-15 12:51:17,781 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"quantized_model\"] = await calculate_throughput(infer, 1000, \"quantized_model\", image_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 13:04:22,347 - Model: tensorrt_model\n",
      "2025-02-15 13:04:22,348 - Batch size: 1\n",
      "2025-02-15 13:04:22,348 - Execution time per cycle: 0.65 ms\n",
      "2025-02-15 13:04:22,349 - Throughput: 1536.34 images/second\n",
      "2025-02-15 13:04:22,349 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# search for \"shape: [\" in the logs\n",
    "image_numpy = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "speed_results[\"tensorrt_model\"] = await calculate_throughput(infer, 1000, \"tensorrt_model\", image_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def infer_bert(\n",
    "    model_name: str,\n",
    "    input_ids: npt.NDArray,\n",
    "    token_type_ids: npt.NDArray,\n",
    "    attention_mask: npt.NDArray,\n",
    "    request_id: str = \"0\",\n",
    "    results: List = None,\n",
    ") -> bool:\n",
    "    if results is None:\n",
    "        results = []\n",
    "    \n",
    "    # Create input tensor\n",
    "    inputs = [grpcclient.InferInput(\"input_ids\", input_ids.shape, \"INT64\"),\n",
    "             grpcclient.InferInput(\"token_type_ids\", token_type_ids.shape, \"INT64\"),\n",
    "             grpcclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT64\")]\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(token_type_ids)\n",
    "    inputs[2].set_data_from_numpy(attention_mask)\n",
    "\n",
    "    # Create output tensor\n",
    "    outputs = [grpcclient.InferRequestedOutput(\"output\")]\n",
    "\n",
    "    # Record the start time for this inference request\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Send inference request with our modified callback that records latency\n",
    "    client.async_infer(\n",
    "        callback=partial(\n",
    "            _async_infer_callback_with_latency,\n",
    "            all_results=results,\n",
    "            start_time=start_time,\n",
    "        ),\n",
    "        model_name=model_name,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        request_id=request_id,\n",
    "    )\n",
    "\n",
    "    # In this example, we immediately return True; results are collected in the callback.\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 21:43:59,361 - Model: bert_onnx_model\n",
      "2025-02-17 21:43:59,364 - Batch size: 1\n",
      "2025-02-17 21:43:59,366 - Execution time per cycle: 114.31 ms\n",
      "2025-02-17 21:43:59,367 - Throughput: 8.75 requests/second\n",
      "2025-02-17 21:43:59,369 - --------------------------------------------------\n",
      "2025-02-17 21:44:11,687 - Model: bert_onnx_model_fp16\n",
      "2025-02-17 21:44:11,689 - Batch size: 1\n",
      "2025-02-17 21:44:11,690 - Execution time per cycle: 123.16 ms\n",
      "2025-02-17 21:44:11,691 - Throughput: 8.12 requests/second\n",
      "2025-02-17 21:44:11,692 - --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.random.randint(0, 10000, (1, 512))\n",
    "token_type_ids = np.random.randint(0, 1, (1, 512))\n",
    "attention_mask = np.random.randint(0, 1, (1, 512))\n",
    "speed_results[\"bert_onnx_model\"] = await calculate_throughput(\n",
    "    infer_bert, 100, \"bert_onnx_model\", input_ids, token_type_ids, attention_mask\n",
    ")\n",
    "speed_results[\"bert_onnx_model_fp16\"] = await calculate_throughput(\n",
    "    infer_bert, 100, \"bert_onnx_model_fp16\", input_ids, token_type_ids, attention_mask\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
